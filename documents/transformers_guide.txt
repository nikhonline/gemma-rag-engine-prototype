# Transformer Neural Networks: The Foundation of Modern LLMs

## Overview

Transformers are a neural network architecture introduced by Vaswani et al. in 2017 that have become the foundation of most modern Large Language Models. Unlike recurrent neural networks (RNNs), transformers process entire sequences in parallel, making them more efficient and scalable.

## Key Innovations

### Self-Attention Mechanism

The self-attention mechanism allows the model to directly compute interactions between all pairs of elements in a sequence, regardless of distance.

**Mechanism:**
1. Each input token is projected into three vectors: Query (Q), Key (K), and Value (V)
2. Attention weights are computed as: Attention(Q, K, V) = softmax(QK^T / √d_k)V
3. This allows each position to attend to all other positions

**Benefits:**
- Captures long-range dependencies
- Parallel computation possible
- Interpretable attention weights

### Multi-Head Attention

Multiple attention heads allow the model to attend to different representation subspaces:

**How it works:**
- Splits attention into h heads with different projections
- Each head learns different aspects of relationships
- Results are concatenated and linearly transformed
- Formula: MultiHead(Q, K, V) = Concat(head₁, ..., headₕ)W^O

**Advantages:**
- Model can focus on different types of relationships
- Improves representation capacity
- Typical values: 8, 12, or 16 heads

## Transformer Architecture

### Encoder-Decoder Structure

**Encoder:**
- Processes input sequence
- Each layer contains: Multi-head Attention → Feed-forward Network
- Produces contextual representations
- Bidirectional: Can see entire input

**Decoder:**
- Generates output sequence
- Each layer contains: Masked Multi-head Attention → Cross-Attention → Feed-forward
- Produces one token at a time (autoregressive)
- Causal masking: Can only attend to previous tokens

### Positional Encoding

Transforms position to represent token order (attention is position-agnostic):

**Sinusoidal Encoding:**
- PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
- PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

**Alternatives:**
- Rotary Position Embeddings (RoPE): More efficient, better extrapolation
- Relative Position Representations: Better handling of variable lengths

### Feed-Forward Networks

Each transformer layer includes a position-wise feed-forward network:

**Structure:**
- Input → Linear(d_model, d_ff) → ReLU → Linear(d_ff, d_model) → Output
- Typically d_ff = 4 × d_model
- Applied independently to each position

**Purpose:**
- Adds non-linearity
- Increases model capacity
- Accounts for majority of parameters

## Important Components

### Layer Normalization

Normalizes inputs across the feature dimension:
- Helps with training stability
- Applied before (Pre-LN) or after (Post-LN) operations
- More stable when applied before (Pre-LN)

### Residual Connections

Skip connections that add original input to processed output:
- Helps gradient flow during backpropagation
- Prevents degradation in very deep networks
- Output = f(x) + x

### Dropout

Regularization technique randomly zeroing activations:
- Prevents overfitting
- Typical rates: 0.1 to 0.2
- Applied during training, disabled during inference

## Model Variants

### Encoder-Only Models
Examples: BERT, RoBERTa
- Process entire sequences bidirectionally
- Good for: Classification, NER, token-level tasks
- Pre-training objective: Masked Language Modeling

### Decoder-Only Models
Examples: GPT, Gemma, Llama
- Generate sequences autoregressively
- Good for: Text generation, question answering
- Pre-training objective: Causal language modeling
- Typically used for LLMs

### Encoder-Decoder Models
Examples: T5, BART
- Combine encoder and decoder
- Good for: Sequence-to-sequence tasks
- Pre-training objectives: Multiple, often span corruption

## Training Process

### Pre-training Objectives

**Causal Language Modeling (CLM):**
- Predict next token based on previous tokens
- Used in GPT-style models
- Objective: Minimize cross-entropy loss

**Masked Language Modeling (MLM):**
- Predict randomly masked tokens
- Used in BERT-style models
- Objective: Minimize prediction error on masked positions

**Next Sentence Prediction (NSP):**
- Predict if two sentences follow each other
- Used in BERT training
- Helps model understand document structure

### Scaling Laws

Empirical relationships between model size, data, and performance:
- Doubling model size: ~3-4% improvement
- Doubling data size: ~2-3% improvement
- Optimal allocation: ~20 tokens per parameter for training data
- Compute budget determines efficient model/data tradeoffs

### Optimization

**Learning Rate Schedules:**
- Warmup: Gradually increase learning rate
- Decay: Decrease learning rate over time
- Typical: Linear warmup followed by cosine decay

**Optimization Algorithms:**
- Adam: Most common, adaptive learning rates
- AdamW: Decouples weight decay from gradient updates
- SGD with momentum: Alternative option

## Efficiency Improvements

### Attention Variants

**Linear Attention:**
- Approximates full attention with linear complexity
- Examples: Performers, Linear Transformers
- Trade-off: Reduced expressiveness

**Sparse Attention:**
- Attention only on subset of tokens
- Reduces O(n²) to O(n log n) or O(n)
- Examples: Longformer, BigBird

**Flash Attention:**
- Improved I/O awareness during attention computation
- Significant speedup without changing computation
- Reduces from O(n² + n) to O(n)

### Model Compression

**Quantization:**
- Reduce precision of weights and activations
- 8-bit, 4-bit, or even 2-bit representations
- Significantly reduces model size and memory

**Pruning:**
- Remove less important connections or heads
- Can achieve 50%+ sparsity with minimal quality loss
- Requires careful selection of what to remove

**Knowledge Distillation:**
- Train smaller "student" model on larger "teacher" model
- Can achieve 80-90% of teacher performance at lower cost

## Gemma Model Architecture

Gemma is Google's open lightweight LLM based on transformer architecture:

**Specifications:**
- Base model: 2B parameters
- Variants: 2B and 7B
- Training data: High-quality internet text
- Architecture: Standard transformer with modern improvements

**Key Features:**
- Efficient inference on consumer hardware
- Safety training aligned with human values
- Open source release under responsible AI terms
- Strong performance despite smaller size

## Conclusion

The transformer architecture has revolutionized deep learning, particularly for NLP. Its combination of self-attention, parallel processing, and scalability has made it the dominant architecture for modern LLMs. Understanding these core concepts is essential for working with and deploying language models effectively.
