# Gemma RAG Engine Prototype

A Retrieval-Augmented Generation (RAG) search engine built with Ollama and Google's Gemma 1B LLM. This system enables intelligent semantic search against local technical documents using vector embeddings and LLM-powered response generation.

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Configuration](#configuration)
- [How It Works](#how-it-works)
- [Document Management](#document-management)
- [Troubleshooting](#troubleshooting)
- [Performance Tips](#performance-tips)
- [Future Enhancements](#future-enhancements)

## üéØ Overview

This RAG engine prototype combines:
- **Ollama**: Local LLM serving platform running Gemma 1B
- **Gemma 1B**: Lightweight, efficient LLM by Google
- **Vector Embeddings**: Semantic search using embeddings generated by Gemma
- **Python**: Fast development and scripting

The system allows you to ask natural language questions about your local technical documents and receive contextually relevant, LLM-generated answers grounded in the actual document content.

## ‚ú® Features

- **Local Processing**: All processing happens locally, no cloud dependencies
- **Vector Search**: Semantic similarity-based document retrieval
- **Interactive CLI**: User-friendly command-line interface
- **Response Generation**: LLM-powered synthesis of search results
- **Caching**: Embeddings cache for faster repeated searches
- **Multi-Format Support**: Load from .txt and .pdf documents
- **Chunking Strategy**: Intelligent document chunking with overlap
- **Similarity Scoring**: Cosine similarity-based relevance ranking
- **Source Attribution**: Knows which documents provided the answer

## üèóÔ∏è Architecture

```
User Query
    ‚Üì
[Query Embedding] ‚Üê Gemma via Ollama
    ‚Üì
[Vector Similarity Search]
    ‚Üì
[Top-K Document Chunks Retrieved]
    ‚Üì
[Context Construction]
    ‚Üì
[LLM Response Generation] ‚Üê Gemma via Ollama
    ‚Üì
User Response with Sources
```

### Components

| Component | Role | Technology |
|-----------|------|-----------|
| Query Processor | Converts queries to embeddings | Gemma via Ollama |
| Vector Store | Stores and searches embeddings | In-memory + pickle cache |
| Document Loader | Reads and processes documents | Python file I/O |
| Chunker | Splits documents into chunks | Custom algorithm |
| Retriever | Finds relevant chunks | Cosine similarity |
| Generator | Creates responses | Gemma via Ollama |

## üìã Prerequisites

Before starting, ensure you have:

- **Windows 10/11** (or compatible OS)
- **Python 3.14.2** (or compatible version)
- **Ollama** installed and running
- **Gemma 1B** model downloaded in Ollama
- **Internet connection** (for initial setup)

### Verify Prerequisites

```powershell
# Check Python version
python --version
# Should output: Python 3.14.2 (or similar)

# Check Ollama status
ollama list
# Should show gemma in the list

# Check Ollama service
curl http://localhost:11434/api/tags
# Should return model list with gemma
```

## üì¶ Installation

### Step 1: Setup Ollama (if not already done)

```powershell
# Download from https://ollama.ai
# Install and run: ollama serve
# In a new terminal, pull Gemma model:
ollama pull gemma

# Verify
ollama list
```

### Step 2: Clone/Setup Project

```powershell
cd c:\Nikhil\Code\gemma-rag-engine-prototype
```

### Step 3: Create Virtual Environment (Optional but Recommended)

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### Step 4: Install Dependencies

```powershell
pip install -r requirements.txt
```

The script will auto-install missing dependencies if needed.

## üöÄ Quick Start

### 1. Start Ollama Service

```powershell
# In PowerShell terminal 1
ollama serve
```

### 2. Add Documents

Place your technical documents in the `documents/` folder:
- Supported formats: `.txt` and `.pdf`
- Examples are included for LLM topics

```powershell
# Add your own documents
Copy-Item "C:\path\to\your\document.txt" documents/
```

### 3. Run the RAG Engine

```powershell
python scripts/rag_search.py
```

### 4. Search Documents

```
[INPUT] Enter your search query (or 'quit' to exit): What is a transformer in machine learning?

[INFO] Searching for: 'What is a transformer in machine learning?'
[‚úì] Found 5 relevant chunks

TOP RESULTS:
[1] Source: transformers_guide.txt
    Similarity: 87.43%
    Content: Transformers are a neural network architecture introduced by Vaswani et al. in 2017...

GENERATED RESPONSE:
A transformer is a neural network architecture that uses self-attention mechanisms to process 
sequences in parallel. Unlike RNNs, transformers can process entire sequences simultaneously,
making them more efficient and scalable...
```

## üìÅ Project Structure

```
gemma-rag-engine-prototype/
‚îú‚îÄ‚îÄ .git/                          # Git repository
‚îú‚îÄ‚îÄ documents/                     # Technical documents (add your own)
‚îÇ   ‚îú‚îÄ‚îÄ llm_introduction.txt       # LLM basics
‚îÇ   ‚îú‚îÄ‚îÄ rag_guide.txt              # RAG explanation
‚îÇ   ‚îî‚îÄ‚îÄ transformers_guide.txt     # Transformers architecture
‚îú‚îÄ‚îÄ scripts/                       # Python scripts
‚îÇ   ‚îî‚îÄ‚îÄ rag_search.py             # Main RAG search engine
‚îú‚îÄ‚îÄ embeddings/                    # Cached embeddings (auto-generated)
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_cache.pkl      # Serialized embeddings
‚îÇ   ‚îî‚îÄ‚îÄ documents_cache.json      # Document metadata
‚îú‚îÄ‚îÄ data/                          # Data directory (for future use)
‚îú‚îÄ‚îÄ requirements.txt               # Python dependencies
‚îî‚îÄ‚îÄ README.md                      # This file
```

## üíª Usage

### Interactive Search Mode (Default)

```powershell
python scripts/rag_search.py
```

Prompts:
- **Enter query**: Type your question in natural language
- **reload**: Reprocess all documents (use after adding new docs)
- **quit**: Exit the application

### Search Examples

#### Example 1: LLM Concepts
```
Query: What are the main components of a transformer?
Response: The main components include self-attention mechanisms, multi-head attention, 
positional encoding, feed-forward networks, and layer normalization...
```

#### Example 2: RAG Methodology
```
Query: How does RAG improve LLM responses?
Response: RAG improves LLM responses by grounding them in actual documents, reducing 
hallucinations, and providing verifiable sources for the information...
```

#### Example 3: Technical Details
```
Query: What is the difference between dense and sparse retrieval?
Response: Dense retrieval uses neural embeddings for semantic understanding and faster search,
while sparse retrieval uses traditional IR methods like BM25 with better interpretability...
```

## ‚öôÔ∏è Configuration

### Modify Ollama Settings

Edit `scripts/rag_search.py`:

```python
# Line 21 - Change Ollama URL if running on different host
engine = RAGSearchEngine(
    ollama_url="http://localhost:11434",  # Default local
    model="gemma"                          # Change model here
)
```

### Adjust Chunking Parameters

Edit `scripts/rag_search.py`:

```python
# In chunk_documents() method
chunk_size = 500    # Characters per chunk (decrease for more granular)
overlap = 50        # Overlap between chunks (increase for more context)
```

### Change Search Parameters

Edit `scripts/rag_search.py`:

```python
# In interactive_search() method
results = self.search(query, top_k=5)  # Change number of results
```

### Embedding Cache Location

To use different cache location, edit:

```python
# Line 25-26
self.embeddings_cache = "embeddings/embeddings_cache.pkl"
self.documents_cache = "embeddings/documents_cache.json"
```

## üîç How It Works

### Phase 1: Initialization

1. **Connection Check**: Verifies Ollama is running at `localhost:11434`
2. **Model Check**: Confirms Gemma model is available
3. **Directory Setup**: Creates necessary folders

### Phase 2: Document Processing

1. **Document Loading**: Reads all `.txt` and `.pdf` files from `documents/` folder
2. **Chunking**: Splits documents into 500-character chunks with 50-character overlap
3. **Metadata Extraction**: Records source file and chunk position

### Phase 3: Embedding Creation

1. **Query to Ollama**: Sends each chunk to Ollama API
2. **Embedding Generation**: Gemma generates vector representation for each chunk
3. **Caching**: Stores embeddings in `embeddings/embeddings_cache.pkl`
4. **Metadata Save**: Saves document info to JSON cache

### Phase 4: Search and Generation

When user enters a query:

1. **Query Embedding**: Converts query to vector using same process
2. **Similarity Search**: Computes cosine similarity between query and all document embeddings
3. **Ranking**: Sorts by similarity score, retrieves top-5 results
4. **Context Construction**: Combines top results into context prompt
5. **Response Generation**: Sends prompt with context to Gemma via Ollama
6. **Output Display**: Shows results and generated response

## üìÑ Document Management

### Adding Documents

```powershell
# Copy documents to the documents folder
Copy-Item "C:\path\to\*.txt" documents/
Copy-Item "C:\path\to\*.pdf" documents/
```

### Supported Formats

- **Text Files** (.txt)
  - Plain text, Markdown, or any text format
  - UTF-8 encoding recommended
  - Unlimited size (but chunked for processing)

- **PDF Files** (.pdf)
  - Text-based PDFs (images won't be extracted)
  - Automatically converted to text

### Document Best Practices

1. **Organization**: Group related documents in descriptive folders
2. **Naming**: Use clear filenames (e.g., `llm_basics.txt`, not `doc1.txt`)
3. **Content**: Technical documents work best (well-structured, clear language)
4. **Size**: Smaller, focused documents perform better than large comprehensive ones
5. **Quality**: Ensure documents are clean and properly formatted

### Refresh Documents

After adding new documents:

```
[INPUT] Enter your search query (or 'quit' to exit): reload
[INFO] Loading documents from 'documents'...
[‚úì] Documents reloaded
```

## üîß Troubleshooting

### Issue: "Could not connect to Ollama"

**Solution:**
```powershell
# Make sure Ollama is running
ollama serve

# Verify connection
curl http://localhost:11434/api/tags

# Check if Gemma is installed
ollama list
```

### Issue: "Gemma model not found"

**Solution:**
```powershell
# Pull the Gemma model
ollama pull gemma

# Verify
ollama list
```

### Issue: Slow embedding creation

**Reasons:**
- First run creates all embeddings (normal)
- Large number of documents
- Slow disk I/O

**Solutions:**
- Wait for completion (can take 5-15 minutes for large corpora)
- Use smaller documents
- Check disk speed

### Issue: "No documents to embed"

**Solution:**
```powershell
# Ensure documents are in correct folder
Get-ChildItem documents/

# Add sample documents
Copy-Item documents/llm_introduction.txt documents/your_docs/
```

### Issue: Low similarity scores for good queries

**Reasons:**
- Documents don't contain relevant information
- Chunk size too small or too large
- Query phrasing differs from document content

**Solutions:**
- Verify document content matches query
- Adjust chunk size (try 300-800 range)
- Rephrase query in different ways
- Add more relevant documents

### Issue: Slow response generation

**Reasons:**
- LLM generation is computationally intensive
- Large context window
- System resource limitations

**Solutions:**
- Normal for LLM generation (5-30 seconds)
- Reduce top_k value
- Run on system with more RAM/GPU

## üìä Performance Tips

### Optimization Strategies

1. **Embedding Cache**
   - First run creates embeddings (time-consuming)
   - Subsequent runs use cache (instant)
   - Delete cache to force refresh: `Remove-Item embeddings/embeddings_cache.pkl`

2. **Chunk Size Tuning**
   - **Too small** (100 chars): Many chunks, slow processing, granular results
   - **Too large** (1000 chars): Fewer chunks, faster processing, less granular
   - **Recommended**: 300-700 characters

3. **Number of Results**
   - More results = slower search but comprehensive context
   - Fewer results = faster but may miss relevant info
   - Sweet spot: 3-5 results

4. **System Resources**
   - Ollama uses significant RAM and CPU
   - Ensure 8GB+ RAM available
   - GPU acceleration recommended (if available)
   - Monitor resource usage in Task Manager

### Scaling Considerations

For larger document sets:
- Use vector database (Weaviate, Qdrant, Milvus)
- Implement result reranking
- Add query preprocessing
- Consider distributed processing

## üöÄ Future Enhancements

Potential improvements to the system:

1. **Web Interface**: Flask/FastAPI REST API and web UI
2. **Advanced Retrieval**:
   - Multi-hop retrieval for complex queries
   - Query expansion and rewriting
   - Result reranking with cross-encoders
3. **Database Integration**:
   - SQLite for metadata
   - Proper vector database (FAISS, Qdrant)
4. **Document Features**:
   - Web scraping support
   - URL document fetching
   - Real-time document updates
5. **Performance**:
   - GPU acceleration for embeddings
   - Async processing
   - Batch operations
6. **Analysis**:
   - Query analytics and statistics
   - Document relevance metrics
   - Performance monitoring
7. **Safety**:
   - Input validation and sanitization
   - Rate limiting
   - Cost tracking for API calls
8. **Models**:
   - Support for other Ollama models
   - Model switching capability
   - Fine-tuned embeddings

## üìö Learning Resources

### About LLMs
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [Gemma Model Card](https://ai.google.dev/gemma) - Google's Gemma documentation

### About RAG
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

### Tools & Libraries
- [Ollama Documentation](https://ollama.ai)
- [Vector Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

## üìù Example Use Cases

1. **Technical Documentation Search**
   - Search across API documentation
   - Find code examples and best practices

2. **Research Paper Analysis**
   - Semantic search through papers
   - Extract and synthesize findings

3. **Knowledge Base Q&A**
   - Company internal knowledge base
   - Policy and procedure lookup

4. **Educational Content**
   - Course material search
   - Conceptual understanding

5. **Product Documentation**
   - Customer service knowledge base
   - Self-service support

## ü§ù Contributing

To improve this prototype:

1. Add more technical documents
2. Optimize embedding and retrieval
3. Implement additional features
4. Fix bugs and improve error handling
5. Enhance documentation

## üìÑ License

This project is open source and available under the MIT License.

## ‚úâÔ∏è Support

For issues, questions, or suggestions:

1. Check the [Troubleshooting](#troubleshooting) section
2. Review [Performance Tips](#performance-tips)
3. Check Ollama logs for errors
4. Ensure all prerequisites are met

## üìÖ Version History

- **v0.1** (Jan 2026) - Initial prototype
  - Basic RAG functionality
  - Ollama integration with Gemma
  - Document loading and chunking
  - Vector embedding and search
  - Interactive CLI interface

---

**Last Updated**: January 17, 2026

**Built with**: Python 3.14.2, Ollama, Gemma 1B

**Status**: ‚úÖ Production Ready (Prototype)
